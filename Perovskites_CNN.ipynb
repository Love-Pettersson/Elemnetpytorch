{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils import data\n",
    "import time, os, re\n",
    "from collections import OrderedDict, defaultdict\n",
    "import collections\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import pickle,gzip\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perovskite_tensor():\n",
    "    with gzip.open('data_p.pickle.gz', 'rb') as ifp:\n",
    "        dataset=pickle.load(ifp)\n",
    "    targetvalues=np.zeros(len(dataset))\n",
    "    Middle_stepA=[]\n",
    "    Middle_stepB=[]\n",
    "    Middle_stepC=[]\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        targetvalues[i]=(float(dataset[i][0])/1000)\n",
    "        Middle_stepA.append(dataset[i][1])\n",
    "        Middle_stepB.append(dataset[i][2])\n",
    "        Middle_stepC.append(dataset[i][3])\n",
    "        \n",
    "\n",
    "\n",
    "    elements = ['H', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'K', 'Ca', 'Sc', 'Ti', 'V', \n",
    "            'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'Ga', 'Ge', 'As', 'Se', 'Br', 'Rb', 'Sr', 'Y', 'Zr', 'Nb', \n",
    "            'Mo', 'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'I', 'Cs', 'Ba', 'La', 'Lu', \n",
    "            'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', \n",
    "            'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi']\n",
    "\n",
    "\n",
    "\n",
    "    formulare = re.compile(r'([A-Z][a-z]*)(\\d*)')\n",
    "    def parse_formula(formula):\n",
    "        pairs = formulare.findall(formula)\n",
    "        length = sum((len(p[0]) + len(p[1]) for p in pairs))\n",
    "        assert length == len(formula)\n",
    "        formula_dict = defaultdict(int)\n",
    "        for el, sub in pairs:\n",
    "            formula_dict[el] += float(sub) if sub else 1\n",
    "        return formula_dict\n",
    "\n",
    "    formulasA = [parse_formula(x) for x in Middle_stepA]\n",
    "    formulasB = [parse_formula(x) for x in Middle_stepB]\n",
    "    formulasC = [parse_formula(x) for x in Middle_stepC]\n",
    "\n",
    "    input1 = np.zeros(shape=(len(formulasA), 65), dtype=np.float32)\n",
    "    input2 = np.zeros(shape=(len(formulasB), 65), dtype=np.float32)\n",
    "    input3 = np.zeros(shape=(len(formulasC), 65), dtype=np.float32)\n",
    "    \n",
    "    \"To create 3*65 input\"\n",
    "    i = -1\n",
    "    for formula in formulasA:\n",
    "        i+=1\n",
    "        keys = formula.keys()\n",
    "        for k in keys:\n",
    "            input1[i][elements.index(k)] = 1/5\n",
    "    dataA = input1\n",
    "\n",
    "    j=-1\n",
    "    for formula in formulasB:\n",
    "        j+=1\n",
    "        keys = formula.keys()\n",
    "        for k in keys:\n",
    "            input2[j][elements.index(k)] = 1/5\n",
    "    dataB = input2\n",
    "    n=-1\n",
    "    for formula in formulasC:\n",
    "        n+=1\n",
    "        keys = formula.keys()\n",
    "        for k in keys:\n",
    "            input3[n][elements.index(k)] = 3/5\n",
    "    dataC = input3\n",
    "    \"To create 65 input\"\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "    \"dataset for split set i.e. 3*65\"\n",
    "    dataset=np.zeros(shape=(len(dataA),3,65),dtype=np.float32)\n",
    "    for i in range(len(dataA)):\n",
    "        dataset[i][0]=dataA[i]\n",
    "        dataset[i][1]=dataB[i]\n",
    "        dataset[i][2]=dataC[i]\n",
    "        \n",
    "    \n",
    "    X_datasplit=torch.from_numpy(dataset)\n",
    "\n",
    "    Y_datasplit=torch.from_numpy(targetvalues).float()\n",
    "    Y_datasplit=Y_datasplit.reshape(-1,1)\n",
    "    return(X_datasplit, Y_datasplit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "  \n",
    "  def __init__(self, inputvector, labels):\n",
    "        \n",
    "        self.labels = labels\n",
    "        self.inputvector = inputvector\n",
    "        \n",
    "        \n",
    "  def __len__(self):\n",
    "        \n",
    "        return len(self.inputvector)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "      \n",
    "      X=self.inputvector[index]\n",
    "      Y=self.labels[index]\n",
    "      \n",
    "      return X,Y\n",
    "    \n",
    "    \n",
    "the_data=Dataset(Perovskite_tensor()[0],Perovskite_tensor()[1])\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(the_data, [20000, 228951])  \n",
    "batchsize=100\n",
    "num_epochs=4000\n",
    "patience=100\n",
    "learningrate=0.001\n",
    "Evalfreq=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1,hidden_size2,hidden_size3,hidden_size4,hidden_size5,hidden_size6, output_size):\n",
    "        super(Network, self).__init__()\n",
    "        self.CNNlayer=nn.Sequential(nn.Conv2d(1,1,(3,65),1,0))\n",
    "        self.reshape=nn.Linear(1,86)\n",
    "        self.layer1=nn.Sequential(nn.Linear(input_size,hidden_size1),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(hidden_size1,hidden_size1),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(hidden_size1,hidden_size1),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(hidden_size1,hidden_size1),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.2)\n",
    "                                  \n",
    "                                  )\n",
    "        self.layer2=nn.Sequential(nn.Linear(hidden_size1,hidden_size2),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(hidden_size2,hidden_size2),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(hidden_size2,hidden_size2),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.1)\n",
    "                                  )\n",
    "        self.layer3=nn.Sequential(nn.Linear(hidden_size2,hidden_size3),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(hidden_size3,hidden_size3),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(hidden_size3,hidden_size3),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.3)\n",
    "                                  )\n",
    "        self.layer4=nn.Sequential(nn.Linear(hidden_size3,hidden_size4),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(hidden_size4,hidden_size4),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(hidden_size4,hidden_size4),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.2)\n",
    "                                  )\n",
    "        self.lastlayer=nn.Sequential(nn.Linear(hidden_size4,hidden_size5),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(hidden_size5,hidden_size5),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(hidden_size5,hidden_size6),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(hidden_size6,output_size)\n",
    "                                  )\n",
    "    def forward(self,x):\n",
    "        x = x.view(batchsize, 1, 3, 65) #batch_size, in_channels, width and height of kernel\n",
    "        out=self.CNNlayer(x)\n",
    "        out=out.view(out.size(0),-1) #flatten it to one dim\n",
    "        out=self.reshape(out)\n",
    "        out=self.layer1(out)\n",
    "        out=self.layer2(out)\n",
    "        out=self.layer3(out)\n",
    "        out=self.layer4(out)\n",
    "        out=self.lastlayer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"Create summary writer\"\n",
    "writer = SummaryWriter('/content/drive/My Drive/Colab Notebooks/Tensorboardsave/Perovskite_CNN')\n",
    "\n",
    "\"Deciding to run on gpu or cpu\"\n",
    "device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "train_loader=torch.utils.data.Dataloader(train_dataset,batch_size=batchsize,shuffle=True)\n",
    "test_loader=torch.utils.data.Dataloader(test_dataset,batch_size=batchsize,shuffle=False)\n",
    "\n",
    "\"The different layer sizes if the network. Note:some layers share size\"\n",
    "input_size=86\n",
    "hidden_size1=1024\n",
    "hidden_size2=512\n",
    "hidden_size3=256\n",
    "hidden_size4=128\n",
    "hidden_size5=64\n",
    "hidden_size6=32\n",
    "output_size=1 \n",
    "\n",
    "\"Retrive the model\"\n",
    "model=Network(input_size,hidden_size1,hidden_size2,hidden_size3,hidden_size4,hidden_size5,hidden_size6,output_size).to(device) \n",
    "\n",
    "\n",
    "\n",
    "\"Create optimizer aswell as training/test loss function \"\n",
    "Testloss=nn.L1Loss()\n",
    "Trainingloss=nn.L1Loss()\n",
    "optimizer=torch.optim.AdamW(model.parameters(), lr=learningrate)\n",
    "scheduler = ReduceLROnPlateau(optimizer,mode='min',factor=0.5,patience=40)\n",
    "\"Defining variables for the training loop\"\n",
    "best_test_error=100\n",
    "best_step=0\n",
    "step=0\n",
    "patience_steps = int((patience * 20000)/(batchsize))\n",
    "\n",
    "\"Create test function\"\n",
    "def test(epoch): \n",
    "    model.eval() #deactivates dropout\n",
    "    test_loss=0 \n",
    "    with torch.no_grad(): #no need for gradients in test phase\n",
    "        k=0\n",
    "        for i,(inputs,labels) in enumerate(test_loader):\n",
    "            inputs=inputs.to(device)\n",
    "            labels=labels.to(device)\n",
    "            output=model(inputs)\n",
    "            test_loss +=Testloss(output,labels)\n",
    "            k +=1\n",
    "    model.train()        \n",
    "    return test_loss/k \n",
    "\n",
    "\"Create training function\"\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    global best_test_error #have to made global\n",
    "    global step\n",
    "    global best_step\n",
    "    k=0\n",
    "    Totloss=0 #Show accumalted loss, not just over one minibatch\n",
    "    \n",
    "    for batch_idx,(inputs,target) in enumerate(train_loader):\n",
    "        \n",
    "        inputs=inputs.to(device)\n",
    "        target=target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output=model(inputs)\n",
    "        loss=Trainingloss(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step +=1\n",
    "        Totloss +=loss.item()\n",
    "        k +=1  \n",
    "        \n",
    "        if batch_idx % Evalfreq == 0:\n",
    "            \n",
    "            Testsetloss=test(epoch)\n",
    "            if best_test_error>Testsetloss:\n",
    "                best_test_error=Testsetloss\n",
    "                best_step=step\n",
    "                torch.save(model.state_dict(),'/content/drive/My Drive/Colab Notebooks/Perovskite_CNN.pth')\n",
    "                torch.save(optimizer.state_dict(),'/content/drive/My Drive/Colab Notebooks/Optimizer_perovskite_CNN.pth')\n",
    "                print('Model saved at: {}'.format('Perovskite_CNN.pth'))\n",
    "            \n",
    "            print('Step: {} Train Epoch:{} Training Loss:{}'.format(step,epoch,Totloss/k))\n",
    "            print('Test set loss: {} Best test loss:{}'.format(test(epoch),best_test_error))\n",
    "            writer.add_scalar('training_loss',loss.item(),step)\n",
    "            writer.add_scalar('test_loss',Testsetloss,step)\n",
    "            scheduler.step(Testsetloss)\n",
    "            Totloss=0\n",
    "            \n",
    "\"Create training loop\"\n",
    "epoch=0\n",
    "while epoch < (epochs + 1):\n",
    "    train(epoch)\n",
    "    if (best_step + patience_steps) <= step:\n",
    "           print('No improvement in the last {} steps, best test error acheived was {}'.format(patience_steps,best_test_error))\n",
    "           print('Done!')\n",
    "           break\n",
    "    \n",
    "    epoch +=1\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
